{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution\n",
    "We start by loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "# Retrieve data\n",
    "news_data = fetch_20newsgroups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"From: lerxst@wam.umd.edu (where's my thing)\\nSubject: WHAT car is this!?\\nNntp-Posting-Host: rac3.wam.umd.edu\\nOrganization: University of Maryland, College Park\\nLines: 15\\n\\n I was wondering if anyone out there could enlighten me on this car I saw\\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\\nthe front bumper was separate from the rest of the body. This is \\nall I know. If anyone can tellme a model name, engine specs, years\\nof production, where this car is made, history, or whatever info you\\nhave on this funky looking car, please e-mail.\\n\\nThanks,\\n- IL\\n   ---- brought to you by your neighborhood Lerxst ----\\n\\n\\n\\n\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See what an element looks like\n",
    "news_data['data'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def compute_tfidf(data):\n",
    "    vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,stop_words='english')\n",
    "    return vectorizer.fit_transform(news_data.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_vec = compute_tfidf(news_data.data)\n",
    "data_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see the details by calling \"vectorizer.vocabulary_\" but there is a lose of values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 4, 4, ..., 3, 1, 8])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_data.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before cross-validation, we need to split the data according to the description of the problem. To do so, we define a helper function split_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "def split_data(ratio_train_testval, ratio_test_val):\n",
    "    # Split the data to have 80% train, 20% test and val\n",
    "    x_train, x_test_val, y_train, y_test_val = train_test_split(data_vec, news_data.target, test_size = ratio_train_testval)\n",
    "    # Split test and val to have 10% each\n",
    "    x_test, x_val, y_test, y_val = train_test_split(x_test_val, y_test_val, test_size = ratio_test_val)\n",
    "    return x_train, y_train, x_test, y_test, x_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test, x_val, y_val = split_data(0.2, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2263, 129791)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(x_test_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9051, 129791)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can cross-validate using the following function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Cross validate hyper parameters using train and validation data\n",
    "def cross_valid():\n",
    "    \n",
    "    n_estim, dep, max_score = 0, 0, 0\n",
    "\n",
    "    for estim in range(55, 65, 5):\n",
    "        for depth in range(155, 170, 5):\n",
    "            regr = RandomForestClassifier(n_estimators=estim, max_depth=depth, random_state = 0)\n",
    "            regr.fit(x_train, y_train)\n",
    "            score = regr.score(x_val, y_val)\n",
    "            print(score)\n",
    "            if(score > max_score):\n",
    "                max_score = score\n",
    "                n_estim = estim\n",
    "                dep = depth\n",
    "    return n_estim, dep, max_score        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.848056537102\n",
      "0.833038869258\n",
      "0.840989399293\n",
      "0.848939929329\n",
      "0.832155477032\n",
      "0.841872791519\n"
     ]
    }
   ],
   "source": [
    "n_estim, dep, max_score = cross_valid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 155, 0.84893992932862195)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(n_estim, dep, max_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how our hyperparameters perform by testing them on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_prediction(n_estimators_, max_depth_):\n",
    "    # Use cross-validated best parameters to predict for test set\n",
    "    regr = RandomForestClassifier(n_estimators=n_estimators_, max_depth=max_depth_, random_state = 0)\n",
    "    X_train = regr.fit(x_train, y_train)\n",
    "    return regr.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85764809902740935"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_prediction(n_estim, dep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is quite a good result. Now we only need to find the most relevant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Returns an array containing the indices of the biggest n elements\n",
    "# in the second array parameter, sorted in descending order\n",
    "def get_n_best_features_idx(n, features_importance):\n",
    "    return np.argsort(features_importance)[::-1][:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the number of best features you want\n",
    "n_best = 20\n",
    "\n",
    "# Computes a dataframe containing the feature names with highest score in descending order\n",
    "def get_n_best_feature_names(n, dictionary, features_importance):\n",
    "    indices = get_n_best_features(n, regr.feature_importances_)\n",
    "    sorted_dict = sorted(list(dictionary.items()), key = lambda tup: tup[1])\n",
    "    names = np.array(list(map(lambda x: x[0],sorted_dict)))[indices]\n",
    "    return pd.DataFrame([names, features_importance[indices]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the two helper functions above, we can simply print the result dataframe to see which words are the most relevant for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>windows</td>\n",
       "      <td>0.00719809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sale</td>\n",
       "      <td>0.00658377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bike</td>\n",
       "      <td>0.00554266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dod</td>\n",
       "      <td>0.00538623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>space</td>\n",
       "      <td>0.00431327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hockey</td>\n",
       "      <td>0.00397577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>car</td>\n",
       "      <td>0.00390783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>encryption</td>\n",
       "      <td>0.00382953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>mac</td>\n",
       "      <td>0.00336992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>god</td>\n",
       "      <td>0.00332612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>clipper</td>\n",
       "      <td>0.00332501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>gun</td>\n",
       "      <td>0.00325418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>israel</td>\n",
       "      <td>0.00316403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>baseball</td>\n",
       "      <td>0.00310418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>israeli</td>\n",
       "      <td>0.00272219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>key</td>\n",
       "      <td>0.00271093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>jesus</td>\n",
       "      <td>0.00259928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>bikes</td>\n",
       "      <td>0.00250856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>graphics</td>\n",
       "      <td>0.00247913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>guns</td>\n",
       "      <td>0.00243553</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0           1\n",
       "0      windows  0.00719809\n",
       "1         sale  0.00658377\n",
       "2         bike  0.00554266\n",
       "3          dod  0.00538623\n",
       "4        space  0.00431327\n",
       "5       hockey  0.00397577\n",
       "6          car  0.00390783\n",
       "7   encryption  0.00382953\n",
       "8          mac  0.00336992\n",
       "9          god  0.00332612\n",
       "10     clipper  0.00332501\n",
       "11         gun  0.00325418\n",
       "12      israel  0.00316403\n",
       "13    baseball  0.00310418\n",
       "14     israeli  0.00272219\n",
       "15         key  0.00271093\n",
       "16       jesus  0.00259928\n",
       "17       bikes  0.00250856\n",
       "18    graphics  0.00247913\n",
       "19        guns  0.00243553"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_n_best_feature_names(n_best, vectorizer.vocabulary_, regr.feature_importances_).transpose()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
